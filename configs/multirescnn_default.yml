paths:
  mimic_dir: &mimic_dir datasets/mimic3/csv
  static_dir: &static_dir datasets/mimic3/static
  dataset_dir: &dataset_dir datasets/mimic3_50
  word2vec_dir: &word2vec_dir datasets/mimic3_50/word2vec
  output_dir: &output_dir results/MultiResCNN_mimic3_50

preprocessing:
  name: mimic_iii_preprocessing_pipeline
  params:
    paths:
      mimic_dir: *mimic_dir
      static_dir: *static_dir
      save_dir: *dataset_dir
      diagnosis_code_csv_name: DIAGNOSES_ICD.csv.gz
      procedure_code_csv_name: PROCEDURES_ICD.csv.gz
      noteevents_csv_name: NOTEEVENTS.csv.gz
      train_json_name: train.json       # will be saved
      val_json_name: val.json           # will be saved
      test_json_name: test.json         # will be saved
      label_json_name: labels.json       # will be computed and saved
    dataset_metadata:
      column_names:
        subject_id: SUBJECT_ID
        hadm_id: HADM_ID
        chartdate: CHARTDATE
        charttime: CHARTTIME
        storetime: STORETIME
        category: CATEGORY
        description: DESCRIPTION
        cgid: CGID
        iserror: ISERROR
        text: TEXT
        icd9_code: ICD9_CODE
        labels: LABELS
    dataset_splitting_method:
      name: caml_official_split
      params:
        hadm_dir: *static_dir
        train_hadm_ids_name: train_split.json
        val_hadm_ids_name: val_split.json
        test_hadm_ids_name: test_split.json
    clinical_note_preprocessing:
      to_lower:
        perform: true
      remove_punctuation:
        perform: true
      remove_numeric:
        perform: true
      remove_stopwords:
        perform: true
        params:
          stopwords_file_path: null
          remove_common_medical_terms: true
      stem_or_lemmatize:
        perform: true
        params:
          stemmer_name: nltk.WordNetLemmatizer
      truncate:
        perform: true
        params:
          max_length: 2000
    code_preprocessing:
      top_k: 50                       # enter 0 for all codes
      code_type: both
      add_period_in_correct_pos:
        perform: true
    tokenizer:
      name: spacetokenizer
      params: null
    embedding:
      name: word2vec
      params:
        embedding_dir: *word2vec_dir
        pad_token: "<pad>"
        unk_token: "<unk>"
        word2vec_params:
          vector_size: 100
          min_count: 3
          epochs: 5

dataset:
  name: base_dataset
  data_common: &data_common
    column_names:
      hadm_id: "HADM_ID"
      clinical_note: "TEXT"
      labels: "LABELS"
    word2vec_dir: *word2vec_dir
    pad_token: "<pad>"
    unk_token: "<unk>"
    dataset_dir: *dataset_dir
    label_file: labels.json
    max_length: 2500
  params:
    train:
      <<: *data_common
      data_file: train.json
    val:
      <<: *data_common
      data_file: val.json
    test:
      <<: *data_common
      data_file: test.json

model:
  name: multirescnn
  # model
  params:
    version: mimic3
    dataset_dir: *dataset_dir
    mimic_dir: *mimic_dir
    static_dir: *static_dir
    word2vec_dir: *word2vec_dir
    embed_file: processed_full.embed
    conv_layer: 1
    num_classes: 50
    kernel_size: 10
    filter_size: "3,5,9,15,19,25"
    test_model: None
    num_filter_maps: 50
    dropout: 0.2
    use_ext_emb: false
    lmbda: 0.0  # Positive for DR-CAML
    gpu : false

trainer:
  name: base_trainer
  params:
    output_dir: *output_dir
    data_loader:
      batch_size: 16
      num_workers: 0
      shuffle: false
      drop_last: true
    loss:
      name: BinaryCrossEntropyLoss
      params: null
    optimizer:
      name: adam_w
      params:
        lr: 0.0001
        weight_decay: 0.0
    max_epochs: 200
    lr_scheduler: 
      name : linearwithwarmup
      params :
        warm_up_proportion: 0.1
        last_epoch: -1  
    stopping_criterion:
      metric:
        name: prec_at_8
      desired: max
      patience: 10
    checkpoint_saver:
      name: base_saver
      params:
        checkpoint_dir: *output_dir
        interval: 1
        max_to_keep: 5
        ckpt_fname_format: "ckpt-{}.pth"
        best_fname_format: "best-{}.pth"
        metric:
          name: prec_at_5
          class: prec_at_k
          params:
            k: 8
        desired: max
    eval_metrics: &eval_metrics
      - name: prec_at_5
        class: prec_at_k
        params:
          k: 5
      - name: prec_at_8
        class: prec_at_k
        params:
          k: 8
      - name: macro_f1
      - name: micro_f1
      - name: macro_auc
      - name: micro_auc
    graph:
      writer:
        name: tensorboard
        params:
          log_dir: *output_dir
      train:
        interval: 100
        interval_unit: step
        metric:
          - name: loss
      val:
        interval: 1
        interval_unit: epoch
        metric:
          - name: loss
          - name: prec_at_5
          - name: prec_at_8
          - name: macro_f1
          - name: micro_f1
          - name: macro_auc
          - name: micro_auc
    seed: 1337
    use_gpu: true
