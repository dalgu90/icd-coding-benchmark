paths:
  mimic_dir: &mimic_dir datasets/mimic3/csv
  static_dir: &static_dir datasets/mimic3/static
  dataset_dir: &dataset_dir datasets/mimic3_50
  word2vec_dir: &word2vec_dir datasets/mimic3_50/word2vec
  cnn_output_dir: &cnn_output_dir results/CNN_mimic3_50
  caml_output_dir: &caml_output_dir results/CAML_mimic3_50
  multirescnn_output_dir: &multirescnn_output_dir results/MultiResCNN_mimic3_50
  fusion_output_dir: &fusion_output_dir results/Fusion_mimic3_50
  dcan_output_dir: &dcan_output_dir results/dcan_mimic3_50

demo:
  icd_desc_file: datasets/mimic3/static/icd9_descriptions.txt
  min_input_len: 4
  top_k: 10
  use_gpu: true
  min_input_len: 4

clinical_note_preprocessing:
  to_lower:
    perform: true
  remove_punctuation:
    perform: true
  remove_numeric:
    perform: true
    replace_numerics_with_letter: null
  remove_stopwords:
    perform: true
    params:
      stopwords_file_path: null
      remove_common_medical_terms: true
  stem_or_lemmatize:
    perform: true
    params:
      stemmer_name: nltk.WordNetLemmatizer
  truncate:
    perform: false

dataset:
  name: base_dataset
  params:
    dataset_dir: *dataset_dir
    word2vec_dir: *word2vec_dir
    pad_token: "<pad>"
    unk_token: "<unk>"
    max_length: 2500
    label_file: labels.json
    data_file: test.json  # not used

models:
  - model:
      name: CNN
      params:
        version: mimic3
        dataset_dir: *dataset_dir
        mimic_dir: *mimic_dir
        static_dir: *static_dir
        word2vec_dir: *word2vec_dir
        num_classes: 50
        embed_size: 100
        kernel_size: 4
        num_filter_maps: 500
        dropout: 0.2
      embed_layer_name: "embed"
    checkpoint_saver:
      name: base_saver
      params:
        checkpoint_dir: *cnn_output_dir
        interval: 1
        max_to_keep: 5
        ckpt_fname_format: "ckpt-{}.pth"
        best_fname_format: "best-{}.pth"
        metric:
          name: prec_at_8
          class: prec_at_k
          params:
            k: 8
        desired: max

  - model:
      name: CAML
      params:
        version: mimic3
        dataset_dir: *dataset_dir
        mimic_dir: *mimic_dir
        static_dir: *static_dir
        word2vec_dir: *word2vec_dir
        num_classes: 50
        embed_size: 100
        kernel_size: 10
        num_filter_maps: 50
        dropout: 0.2
        lmbda: 0.0  # Positive for DR-CAML
        init_code_emb: false
        pad_token: "<pad>"
        unk_token: "<unk>"
      embed_layer_name: "embed"
    checkpoint_saver:
      name: base_saver
      params:
        checkpoint_dir: *caml_output_dir
        interval: 1
        max_to_keep: 5
        ckpt_fname_format: "ckpt-{}.pth"
        best_fname_format: "best-{}.pth"
        metric:
          name: prec_at_8
          class: prec_at_k
          params:
            k: 8
        desired: max

  - model:
      name: multirescnn
      params:
        version: mimic3
        dataset_dir: *dataset_dir
        mimic_dir: *mimic_dir
        static_dir: *static_dir
        word2vec_dir: *word2vec_dir
        embed_file: processed_full.embed
        conv_layer: 1
        num_classes: 50
        filter_size: [3, 5, 9, 15, 19, 25]
        test_model: None
        num_filter_maps: 50
        dropout: 0.2
        use_ext_emb: false
      embed_layer_name: "word_rep"
    checkpoint_saver:
      name: base_saver
      params:
        checkpoint_dir: *multirescnn_output_dir
        interval: 1
        max_to_keep: 5
        ckpt_fname_format: "ckpt-{}.pth"
        best_fname_format: "best-{}.pth"
        metric:
          name: prec_at_8
          class: prec_at_k
          params:
            k: 8
        desired: max

  - model:
      name: dcan
      params:
        num_classes: 50
        word_representation_layer:
          params:
            init_params:
              embed_dir: *word2vec_dir
              dropout: 0.2
            freeze_layer: true
        add_emb_size_to_channel_sizes: true
        conv_channel_sizes: [[600, 600], [600, 600, 600], [600, 600, 600]]
        kernel_sizes: [[2, 2], [2, 2], [2, 2]]
        strides: [[1, 1], [1, 1], [1, 1]]
        dilations: [[1, 1], [2, 2], [4, 4]]                 # 2^(temporal_conv_net_level)
        paddings: [[1, 1], [2, 2], [4, 4]]                  # (kernel_size - 1) * dilation_size
        dropout: 0.2
        weight_norm: true
        projection_size: 300
        activation: "relu"
      embed_layer_name: "word_embedding_layer"
    checkpoint_saver:
      name: base_saver
      params:
        checkpoint_dir: *dcan_output_dir
        interval: 1
        max_to_keep: 5
        ckpt_fname_format: "ckpt-{}.pth"
        best_fname_format: "best-{}.pth"
        metric:
          name: prec_at_8
          class: prec_at_k
          params:
            k: 8
        desired: max

  - model:
      name: Fusion
      params:
        version: mimic3
        dataset_dir: *dataset_dir
        mimic_dir: *mimic_dir
        static_dir: *static_dir
        word2vec_dir: *word2vec_dir
        num_classes: 50
        dropout: 0.2
        filter_size: [3, 5, 9, 15, 19, 25]
        use_attention_pool: true
        pool_size: 2
        conv_layer: 1
        use_layer_norm: true
        use_relu: false
        use_transformer: true
        max_length: 2500
        transfer_layer: 1
        transfer_attention_head: 4
        transfer_fsize: 1024
        num_filter_maps: 50
      embed_layer_name: "word_rep"
    checkpoint_saver:
      name: base_saver
      params:
        checkpoint_dir: *fusion_output_dir
        interval: 1
        max_to_keep: 5
        ckpt_fname_format: "ckpt-{}.pth"
        best_fname_format: "best-{}.pth"
        metric:
          name: prec_at_8
          class: prec_at_k
          params:
            k: 8
        desired: max
