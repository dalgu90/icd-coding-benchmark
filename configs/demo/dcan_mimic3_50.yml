paths:
  mimic_dir: &mimic_dir datasets/mimic3/csv
  static_dir: &static_dir datasets/mimic3/static
  dataset_dir: &dataset_dir datasets/mimic3_50
  word2vec_dir: &word2vec_dir datasets/mimic3_50/word2vec
  output_dir: &output_dir results/dcan_mimic3_50

demo:
  icd_desc_file: datasets/mimic3/static/icd9_descriptions.txt
  min_input_len: 4
  top_k: 10
  use_gpu: true

clinical_note_preprocessing:
  to_lower:
    perform: true
  remove_punctuation:
    perform: true
  remove_numeric:
    perform: true
    replace_numerics_with_letter: null
  remove_stopwords:
    perform: true
    params:
      stopwords_file_path: null
      remove_common_medical_terms: true
  stem_or_lemmatize:
    perform: true
    params:
      stemmer_name: nltk.WordNetLemmatizer
  truncate:
    perform: false

dataset:
  name: base_dataset
  params:
    dataset_dir: *dataset_dir
    word2vec_dir: *word2vec_dir
    pad_token: "<pad>"
    unk_token: "<unk>"
    max_length: 2500
    label_file: labels.json
    data_file: test.json  # not used

model:
  name: dcan
  params:
    num_classes: 50
    word_representation_layer:
      params:
        init_params:
          embed_dir: *word2vec_dir
          dropout: 0.2
        freeze_layer: true
    add_emb_size_to_channel_sizes: true
    conv_channel_sizes: [[600, 600], [600, 600, 600], [600, 600, 600]]
    kernel_sizes: [[2, 2], [2, 2], [2, 2]]
    strides: [[1, 1], [1, 1], [1, 1]]
    dilations: [[1, 1], [2, 2], [4, 4]]                 # 2^(temporal_conv_net_level)
    paddings: [[1, 1], [2, 2], [4, 4]]                  # (kernel_size - 1) * dilation_size
    dropout: 0.2
    weight_norm: true
    projection_size: 300
    activation: "relu"
  embed_layer_name: "word_embedding_layer"

checkpoint_saver:
  name: base_saver
  params:
    checkpoint_dir: *output_dir
    interval: 1
    max_to_keep: 5
    ckpt_fname_format: "ckpt-{}.pth"
    best_fname_format: "best-{}.pth"
    metric:
      name: prec_at_8
      class: prec_at_k
      params:
        k: 8
    desired: max
